#+LATEX_HEADER:  \usepackage{amssymb}
#+LATEX_HEADER_EXTRA: \newcommand\ssum[1]{\sum_{\substack{#1}}}
#+LATEX_HEADER_EXTRA: \newcommand\E{\mathbb{E}}
#+OPTIONS: toc:nil
* Intro
Today in class we will dicuss random graph models and doing statisitics on such graphs.

* Statistics Review
 Stats review
 *Weak Law of Large Number* Consider x_{i}|_{1}^{n} i.i.d. random variables with E[x_{i}]= \mu then for all \epsilon > 0
 P( |\sum x_{i} - n \mu | > \epsilon \mu ) \to 0  as n \to \infty
 Also known as asymptotically almost surely or a.a.s.

 *Strong Law* P(lim | mean(x) - \mu| > 0 )

 *Sterling Approximation* For any natural number n,
   \sqrt{2\pi n} (n/e)^{n} \leq n! \leq e\sqrt{n} (n/e)^{n}
   Therefore \dbinom{n}{k} \leq (en/k)^{k}

Exponential: lim_{n\to \infty} (1 - x/n)^{n} = x^{n}

** Linearity of Expectation
A important useful fact to know is that if we have a set of n random variables, $X_{i}$ then if we define a new variabe $Y = \sum_{i=1}^{n} X_{i}$ then linearity of expectation says that $E[Y] = E[\sum X_{i}] = \sum E[X_{i}]$. In practice, we will often have the $E[X_{1}] = E[X_{2}] = \cdots = E[X_{n}]$ in which case $E[Y] = n E[X_{1}]$. Also note this is still true even if the random variables $X_{i}$ are not independent of each other.

A common pattern applied often is to find the expectation of a random variable Y, let $X_{i}$ be the indicator of some event i.e. $X_{i} = \begin{cases} 1 & \text{ If event i happens} \\ 0 & \text{ else} \end{cases}$. In such a case where $Y = \sum X_{i}$ we would have $E[Y] = \sum E[X_{i}] = \sum 1*Pr[X_{i}]$.


** Common Distributions

This is just a table of some common distributions and the basic facts about them. This is hopefully mostly reference.
| Distribution | definition                                                                        | E[X] ] | Var[X]  |
| Bernoulli    | Pr[X = 1] = p, Pr[X=0]=1-p                                                        | p      | p(1-p)  |
| Binomial     | Pr[X=k]  = \binom{n}{k} p^k (1-p)^{n-k}                                              | np     | np(1-p) |
| Poisson      | Pr[X=x] \frac{e^{-\lambda} \lambda^{x}}{x!}                                                         | \lambda      | \lambda       |
| Normal       | Pr[X=x] =  \frac{1}{(2\pi)^{\frac{1}{2}}\sigma} \left[ \frac{-1}{2} \frac{x -\mu}{\sigma}^{2} \right] | \mu      | \sigma^{2}      |
This is the simplest model where we have a random variable $X_{i}$ and $Pr[X_{i} = 1] = p$ and $Pr[X_{i} = 0] = 1-p$.

* Erdos-Renyi Random Graphs
The first graph model we introduce is also the most intuitive for producing a random undirected graph.  The Erdos-Renyi random graph model takes two parameters, the number of nodes and probability that there is an edge between two vertices This probablity is uniform across every pair of nodes. In the following we will let G(n,p) denote an Erdos-Renyi(ER for the rest of these notes) graph with n nodes and link probablity p.


To begin saying interesting things about these graphs we will define the Bernoulli random variables $\ell_{ij} = \begin{cases} 1 & \text{i is connected to j} \\ 0 & \text{Otherwise} \end{cases}$ for all $i\neq j, i,j \in [n]. Notice that because the link probablity is uniform the set of all $\ell_{ij}$ variables are independent and specifically $Pr[\ell_{ij} = 1] = p$.

** Number of Edges in ER graphs
This leads to the following expression the expected number of edges.
\begin{align}
    \E[\# Edges] &= \E[\sum_{\substack{i,j \in [n] \\ i\neq j}} l_{ij}  ] \\
    &= \ssum{ {i,j \in [n] \\ i\neq j} }  \E [l_{ij}] \\
    &= \binom{n}{2} p \\
    &= \frac{n(n-1)}{2} p
\end{align}


Using the weak law of large numbers, we have that for any $\alpha > 0$ we have
$$Pr[\left| \ssum{i \neq j} l_{ij} - p \frac{n(n-1)}{2} \right|  > \alpha p \frac{n(n-1)}{2} ] \to 0$$
as $n \to \infty$.

The implication of this is that for large n the odds we have a random graph that doesn't have close to $\binom{n}{2}p$ goes to 0.

** Degrees of ER Graphs
Now we will figure out what the expected degree of any node v in an ER graph is. Let D be a random variable representing the degree of v.
\begin{align}
\E[D] &= \E[\ssum{j\neq v} l_{{vj} }] \\
&= \ssum{j\neq v} \E [ l_{vj} ] \\
&= (n-1)*p
\end{align}

And the probablity D=d is the well known probablity distrubtion of a binomial random variable. That is $Pr[D=d] = \binom{n-1}{d} p^{d} (1-p)^{n-1-p}$ where we use n-1 instead of n because there are n-1 nodes that could match with our target node since self loops aren't permitted.

If we fixed the expected degree with $\lambda = (n-1)p$ then we can approximate this has $Pr[D=d] = \frac{e^\lambda \lambda^{d}}{d!}$



** Individual clustering coefficient
Clustering coefficient of an ER graph is Cl_{i}(p) = p
*I don't know how get this with what was in class*

* Transistion functions
A thing we will commonly look at in this class is how a property of choosen graph family changes as the graph becomes bigger and the link probablity changes with it. Usually the probablity will of links decreases with n as we are frequently interested in properties of sparse graphs.

Instead of doing the challenging working of fully analzying the probablity of a given property for a fixed n, it will typically be easier to asymptomatic analysis on what happens as $n\to \infty$. We will often see that the probablity of an event goes to 0 if the link probablity p(n) shrinks strictly faster than some specicial threshold function and to 1 if p(n) shrinks slower. We call such behavior a phase transistion.

** Example
The function $f(n) = \frac{log(n)}{n}$ clearly goes to zero as $n \to \infty$. It can be shown that if $p(n) / f(n) \to 0$ with n then odds the graph is connected goes to zero but if $p(n)/f(n) \to \infty$ then the graph will be connected with incredibly high probablity.

1. Define A = number of edges > 0
  Threshold of emergence of the first edge E[#Edges] = C(n,2) * p \approx n^{2}/2 p(n)
   So is p*n^{2}/2 \to 0 then E[number of edges] \to 0 so P(number of edges) \to 0

   If instead p(n)n^{2}/2 then E[number of edge] \to \infty
   *Remark* Does not imply that P(number of edges > 0 ) \to 1

2. 1/n^{2} is the threshold function for the emergence of the first link.

3. E[number of triples] = n^{3} p^{2} using a similar analysis we can show that t(n) = n^{-3/2} is a threshold function
4. Threshold for observing a cycle with k nodes is t(n) = 1/n
5. Also at 1/n the largest component has O(log(n)) nodes
6. At p(n)= log(n)/n threshold the network becomes connected
